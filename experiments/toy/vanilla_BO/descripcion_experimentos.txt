Quería describir, por un lado, los baselines contra los que quería comparar el rendimiento de la transformación biyectiva. Por otro lado, la transformación biyectiva. Supongamos que la función objetivo a minimizar es una muestra del GP (obtenida por random features) en dos dimensiones. También haré de 3D y 4D. Cada punto de los siguientes corresponde a un método.

    Transformación biyectiva: Optimiza en 2D. Para saber qué punto del simplex (3 dimensiones) corresponde a su representación en dos dimensiones se hace la transformación biyectiva. Esto es lo que mejor debería funcionar ya que solo tenemos dos dimensiones en la optimización y encima hay match del modelo.

    Transformación del hipercubo al simplex: Esta es la primera transformación que comentamos. Debo comparar su rendimiento con respecto a la misma función objetivo que la transformación biyectiva. Sin embargo, esta transformación trabajaría en 3D y no en 2D (en este ejemplo). Por ello, hago dos pasos: Primero, transformo el punto sugerido en 3D de la optimización Bayesiana al simplex con la transformación que planteamos inicialmente (esta transformación NO es biyectiva). Luego, uso la inversa de la transformación biyectiva para extraer el valor de la función objetivo. Debería funcionar peor que la transformación biyectiva, ya que el GP de la optimización Bayesiana aquí tiene 3D y habría muchos puntos que se converitirían en el mismo. 

    Penalización según se aleja del simplex un punto en 3D: Debe funcionar mucho peor pero es lo que vi por internet que hacían para resolver este problema. Luego se puede citar la fuente y demostrar que nuestras transformaciones van mucho mejor. Para implementar esto en problemas sintéticos, lo que voy a hacer es extraer el peor punto del simplex en 2D. Posteriormente, si la BO me recomienda un punto fuera del simplex, calculo su distancia mínima al simplex y penalizo linealmente para hacer que la función de adquisición sea suave. Si pongo para cualquier punto que yace fuera del simplex siempre un valor constante peor que el peor valor posible del simplex entonces la BO no podría "aprender" donde se encuentra el simplex por las observaciones. Sin embargo, al final de la optimización, el resultado final del valor sino encuentra la BO un punto en el simplex sería INVALIDO.

    Búsqueda aleatoria en el simplex: Optimiza en 2D. Igual que el primer punto. Pero en vez de usar BO, usamos búsqueda aleatoria.

    Búsqueda aleatoria en el hipercubo: Igual que el segundo punto. Pero en vez de usar BO, usamos búsqueda aleatoria.
